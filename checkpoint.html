<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <title>DB-LoadBalancer by amaliujia</title>
  </head>

  <body>

    <header>
      <div class="container">
        <h1>Key-Value DB Load Balancer</h1>
        <h2>Database, Parallel Computing, Load Balance</h2>

        <section id="downloads">
          <a href="https://github.com/amaliujia/DB-LoadBalancer/zipball/master" class="btn">Download as .zip</a>
          <a href="https://github.com/amaliujia/DB-LoadBalancer/tarball/master" class="btn">Download as .tar.gz</a>
          <a href="https://github.com/amaliujia/DB-LoadBalancer" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        <h2>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Checkpoint Report</h2>


<p><h3>Experiment Design</h3>
In this phrase, we had a basic design of our experiment. We decided what benchmark we use, what kind of trace files should produce and what kind of machines we can use.<br/> <br/> 

Firstly, we got permission on usage of LTIâ€™s cluster. This cluster has four nodes. Each node has two, four core Xeon E5345 processor (<a href="http://ark.intel.com/products/28032/Intel-Xeon-Processor-E5345-8M-Cache-2_33-GHz-1333-MHz-FSB?q=E5345">2.33GHz, 8M L2 cache, no hyper-threading</a>), 16GB memory and hard disk(15000RPM). Based on hardware of machines, we decided to use one node as Load-Balancer, in the meantime, every node was installed a distribution of Emeralddb. <br/> <br/> 

The procedure of experiment is, each time, we implement a kind of schedule policy, and run this prolicy with two different trace files on 1, 2 and 3 nodes, respectively. For now Evaluation is based on how many times speedup acquired on numbers of operations per unit time. For instance, if 1 node has 100 op/sec and 4 nodes have 400op/s, we say 4x speedup is acquired. The detail of schedule policy, benckmark, traces will talk about later. </p>

<p><h3>Schedule Policy</h3>
For now we come up with two policy:<br/><br/>

Equalsharing policy: The purpose of this policy, is to maintain theoretically balanced workload on each node. So for insert operation, it will be assgined to node which has least number of records. For other operations, like query and delete, will be transfered to node who has this record. This is a greedy policy, and require more memory and disk space for load balancer compared to Naivesharing policy, becasue this policy needs to maintain mapping between records and nodes, based on unique key of record. For now, in memory hashtable is used, but in real word, such important index should be saved in disk or ssd, along with log, to keep metadata integrity.<br/> <br/> 

Equalsharing with multi-client policy: This policy is similar to Equalsharing policy, along with multi-client access to one instance of DB. So it tries to utilize multi-threading to improve performance. More specifically, this time, for every DB instance, we have more than one client to send requests to DB. These clients share a request queue.
 </p>

<p><h3>Benchmark</h3>
The baseline is, performance of trace runs on single node. Expected performance of load balaner is, to gain linear speedup. As said before, speedup is based on comparing numbers of operations per unit time in different cases.</p> 

<p><h3>Trace Files</h3>
We produce two different kinds of trace.
<ul>
<li>Single Insert trace: this trace consists of 300000 insert operation with random generated key.</li>

<li>Mixd operations trace:this trace consists of 1000000 operation tuples. Each tuple is like (Insert, query, delete), and each tuple share the same key.</li>
</ul>
 </p>
 
<p><h3>Current progress</h3>
<Strong>EqualSharing Policy</Strong>
<img src="img1.png"><br/><br/>

<Strong>EqualSharing with multi-client policy</Strong>
<img src="img2.png"><br/><br/>

<Strong>Comparison of two policy</Strong>
<img src="img4.png"><br/><br/>

</p>

<p><h3>Current observations</h3>
Based on experiment, we can tell that different policy has significant different performance. The main contribution of this is multi-threading. However, for each policy, multi nodes cannot help improve performance than single node. Even worse, in Equalsharing with multi-client policy, performance of multi nodes is worse than sinlge node.
</p>

 
 <p><h3>Issue with current progress</h3>
As said before, multi nodes cannot help improve performance than single node. We guess the reason maybe is multi-threading has queue and logger contention, because each thread has to write something into logger and catch requests from queue. Such contention results in serialization. <br/> <br/> 

Although we have some guess, we still cannot identify what is bottleneck of our load-balancer. So what is the reason we cannot achieve linear speedup? Such problems still need to be explored in the next two weeks.<br/> <br/> 

 </p>

      </section>
    </div>

    
  </body>